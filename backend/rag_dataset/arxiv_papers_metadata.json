[
  {
    "title": "KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing",
    "authors": [
      "Rui Li",
      "Quanyu Dai",
      "Zeyu Zhang",
      "Xu Chen",
      "Zhenhua Dong",
      "Ji-Rong Wen"
    ],
    "published": "2025-05-26T17:22:20+00:00",
    "summary": "Recent advances in retrieval-augmented generation (RAG) furnish large\nlanguage models (LLMs) with iterative retrievals of relevant information to\nhandle complex multi-hop questions. These methods typically alternate between\nLLM reasoning and retrieval to accumulate external information into the LLM's\ncontext. However, the ever-growing context inherently imposes an increasing\nburden on the LLM to perceive connections among critical information pieces,\nwith futile reasoning steps further exacerbating this overload issue. In this\npaper, we present KnowTrace, an elegant RAG framework to (1) mitigate the\ncontext overload and (2) bootstrap higher-quality multi-step reasoning. Instead\nof simply piling the retrieved contents, KnowTrace autonomously traces out\ndesired knowledge triplets to organize a specific knowledge graph relevant to\nthe input question. Such a structured workflow not only empowers the LLM with\nan intelligible context for inference, but also naturally inspires a reflective\nmechanism of knowledge backtracing to identify contributive LLM generations as\nprocess supervision data for self-bootstrapping. Extensive experiments show\nthat KnowTrace consistently surpasses existing methods across three multi-hop\nquestion answering benchmarks, and the bootstrapped version further amplifies\nthe gains.",
    "filename": "arxiv_2505.20245_v1.pdf",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent",
    "authors": [
      "Jiahao Qiu",
      "Fulian Xiao",
      "Yimin Wang",
      "Yuchen Mao",
      "Yijia Chen",
      "Xinzhe Juan",
      "Siran Wang",
      "Xuan Qi",
      "Tongcheng Zhang",
      "Zixin Yao",
      "Jiacheng Guo",
      "Yifu Lu",
      "Charles Argon",
      "Jundi Cui",
      "Daixin Chen",
      "Junran Zhou",
      "Shuyao Zhou",
      "Zhanpeng Zhou",
      "Ling Yang",
      "Shilong Liu",
      "Hongru Wang",
      "Kaixuan Huang",
      "Xun Jiang",
      "Yuming Cao",
      "Yue Chen",
      "Yunfei Chen",
      "Zhengyi Chen",
      "Ruowei Dai",
      "Mengqiu Deng",
      "Jiye Fu",
      "Yunting Gu",
      "Zijie Guan",
      "Zirui Huang",
      "Xiaoyan Ji",
      "Yumeng Jiang",
      "Delong Kong",
      "Haolong Li",
      "Jiaqi Li",
      "Ruipeng Li",
      "Tianze Li",
      "Zhuoran Li",
      "Haixia Lian",
      "Mengyue Lin",
      "Xudong Liu",
      "Jiayi Lu",
      "Jinghan Lu",
      "Wanyu Luo",
      "Ziyue Luo",
      "Zihao Pu",
      "Zhi Qiao",
      "Ruihuan Ren",
      "Liang Wan",
      "Ruixiang Wang",
      "Tianhui Wang",
      "Yang Wang",
      "Zeyu Wang",
      "Zihua Wang",
      "Yujia Wu",
      "Zhaoyi Wu",
      "Hao Xin",
      "Weiao Xing",
      "Ruojun Xiong",
      "Weijie Xu",
      "Yao Shu",
      "Xiao Yao",
      "Xiaorui Yang",
      "Yuchen Yang",
      "Nan Yi",
      "Jiadong Yu",
      "Yangyuxuan Yu",
      "Huiting Zeng",
      "Danni Zhang",
      "Yunjie Zhang",
      "Zhaoyu Zhang",
      "Zhiheng Zhang",
      "Xiaofeng Zheng",
      "Peirong Zhou",
      "Linyan Zhong",
      "Xiaoyin Zong",
      "Ying Zhao",
      "Zhenxin Chen",
      "Lin Ding",
      "Xiaoyu Gao",
      "Bingbing Gong",
      "Yichao Li",
      "Yang Liao",
      "Guang Ma",
      "Tianyuan Ma",
      "Xinrui Sun",
      "Tianyi Wang",
      "Han Xia",
      "Ruobing Xian",
      "Gen Ye",
      "Tengfei Yu",
      "Wentao Zhang",
      "Yuxi Wang",
      "Xi Gao",
      "Mengdi Wang"
    ],
    "published": "2025-05-26T17:22:20+00:00",
    "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.",
    "filename": "arxiv_2505.20246_v1.pdf",
    "categories": [
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "title": "It's High Time: A Survey of Temporal Information Retrieval and Question Answering",
    "authors": [
      "Bhawna Piryani",
      "Abdelrahman Abdullah",
      "Jamshid Mozafari",
      "Avishek Anand",
      "Adam Jatowt"
    ],
    "published": "2025-05-26T17:21:26+00:00",
    "summary": "Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nInformation Retrieval and Temporal Question Answering, two research areas aimed\nat handling and understanding time-sensitive information. As the amount of\ntime-stamped content from sources like news articles, web archives, and\nknowledge bases increases, systems must address challenges such as detecting\ntemporal intent, normalizing time expressions, ordering events, and reasoning\nover evolving or ambiguous facts. These challenges are critical across many\ndynamic and time-sensitive domains, from news and encyclopedias to science,\nhistory, and social media. We review both traditional approaches and modern\nneural methods, including those that use transformer models and Large Language\nModels (LLMs). We also review recent advances in temporal language modeling,\nmulti-hop reasoning, and retrieval-augmented generation (RAG), alongside\nbenchmark datasets and evaluation strategies that test temporal robustness,\nrecency awareness, and generalization.",
    "filename": "arxiv_2505.20243_v1.pdf",
    "categories": [
      "cs.CL",
      "cs.IR"
    ]
  }
]